---
# tasks file for roles/hpl

- name: Install MPI, ATLAS and required dependencies
  apt:
    pkg:
    - automake
    - gfortran
    - git
    - libatlas-base-dev
    state: latest

- name: Disable CPU throttling - lasts until reboot
  shell: "echo performance | sudo tee /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor"

# Using the repo MPI libs resulted in segmentation faults when running HPL.
# So building MPI from source.
- name: Make a working directly for MPI
  file:
    name: "{{ mpi_working_dir }}"
    state: directory
    mode: '0755'
    owner: pi
    group: pi

- name: Get MPI source code
  get_url:
    url: "{{ mpi_download_url }}/{{ mpi_file_version }}/mpich-{{ mpi_file_version }}{{ mpi_file_extension }}"
    dest: "{{ mpi_working_dir }}"

- name: Untar MPI file
  unarchive:
    src: "{{ mpi_working_dir }}/mpich-{{ mpi_file_version }}{{ mpi_file_extension }}"
    dest: "{{ mpi_working_dir }}"
    remote_src: yes
  become_user: pi

# ch4 device is a newer communications protocol. In theory should have some advantages here, but takes a lot longer to build.
# Switch to ch3 for a faster build. ./configure --with-device=ch3:sock
# Long running make jobs disconnect - verbose shows 'sftp transfer mechanism failed'. Maybe docker related?
# So using async mode, will keep trying for 1200sec (20min) every 60s
- name: Setup for MPI make
  command: "./configure --with-device=ch4:ofi"
  args:
    chdir: "{{ mpi_working_dir }}/mpich-{{ mpi_file_version }}"
  become_user: pi
  async: 1200
  poll: 60

- name: Make MPI
  make:
    chdir: "{{ mpi_working_dir }}/mpich-{{ mpi_file_version }}"
    params:
      NUM_THREADS: 4
  become_user: pi
  async: 7200
  poll: 60

- name: Make install MPI
  make:
    chdir: "{{ mpi_working_dir }}/mpich-{{ mpi_file_version }}"
    target: install
  async: 3600
  poll: 60

# The repo ATLAS works ok. If not working, or not performant, then build once, store centrally, 
# and pull from a store e.g. github or a file/object store.
# - name: Get ATLAS pre-built - make directory for repo
#   file:
#     name: "{{ atlas_working_dir }}"
#     state: directory
#     mode: '0755'
#     owner: pi
#     group: pi

# - name: Add github to known_hosts
#   shell: "ssh-keyscan -H github.com >> ~/.ssh/known_hosts"
#   become_user: pi

# - name: Get ATLAS pre-built - clone repo to dir
#   git:
#     repo: <REPO>
#     dest: "{{ atlas_working_dir }}"
#   become_user: pi

- name: Make a working directly for HPL
  file:
    name: "{{ hpl_working_dir }}"
    state: directory
    mode: '0755'
    owner: pi
    group: pi

- name: Get HPL source code
  get_url:
    url: "{{ hpl_download_url }}{{ hpl_file_version }}{{ hpl_file_extension }}"
    dest: "{{ hpl_working_dir }}"

- name: Untar hpl file
  unarchive:
    src: "{{ hpl_working_dir }}/{{ hpl_file_version }}{{ hpl_file_extension }}"
    dest: "{{ hpl_working_dir }}"
    remote_src: yes
  become_user: pi

- name: Setup for HPL make
  command: "sh make_generic"
  args:
    chdir: "{{ hpl_working_dir }}/{{ hpl_file_version }}/setup"
  become_user: pi

- name: Copy in Make.rpi
  template:
    src: templates/Make.rpi.j2
    dest: "{{ hpl_working_dir }}/{{ hpl_file_version }}/Make.rpi"
  become_user: pi

- name: Make HPL
  make:
    chdir: "{{ hpl_working_dir }}/{{ hpl_file_version }}"
    params:
      arch: rpi
  become_user: pi
  async: 7200
  poll: 60

- name: Add all nodes in cluster to trusted hosts
  shell: "ssh-keyscan -H {{ item }} >> ~/.ssh/known_hosts"
  with_inventory_hostnames:
    - current
  become_user: pi

- name: Backup original HPL.data
  copy:
    src: "{{ hpl_working_dir }}/{{ hpl_file_version }}/bin/rpi/HPL.dat"
    dest: "{{ hpl_working_dir }}/{{ hpl_file_version }}/bin/rpi/HPL.dat.ori"
    remote_src: yes
  become_user: pi

- name: Add HPL.dat as a new copy
  template:
    src: templates/HPL.dat.j2
    dest: "{{ hpl_working_dir }}/{{ hpl_file_version }}/bin/rpi/HPL.dat.new"
  become_user: pi

- name: Add nodes-Xpi
  vars:
    node_list: "{{ groups['current'] }}"
  template:
    src: templates/nodes-Xpi.j2
    dest: "{{ hpl_working_dir }}/{{ hpl_file_version }}/bin/rpi/nodes-Xpi"
  become_user: pi

- debug:
    msg: "To test 1 nodes cd {{ hpl_working_dir }}/{{ hpl_file_version }}/bin/rpi/; mpirun -np 4 ./xhpl"

- debug:
    msg: "To run 4 nodes: cd {{ hpl_working_dir }}/{{ hpl_file_version }}/bin/rpi/; mpiexec -f nodes-Xpi ./xhpl"

- debug:
    msg: "If reboot/shutdown, then need to repeat disable CPU throttle"
